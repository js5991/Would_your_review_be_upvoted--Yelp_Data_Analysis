{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These code are written in python 2.6 and sklearn 0.12.1 to run on the server, more like a record in the jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the package needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the dataset (replace it with the real training later) and drop index col, review_id and votes_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/lx557/final_getdummies_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head(1)#see which should be dropped\n",
    "data=data.drop([data.columns[0],'review_id','votes_total_user'],1)#should leave 31 columns\n",
    "data=data.drop(data.columns[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Shape(913199, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split it into two parts, the first trains the model, the second picks up the best feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data,data['Label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n",
    "((730559, 31), (182640, 31), (730559,), (182640,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(X_train,columns=data.columns)\n",
    "X_test=pd.DataFrame(X_test,columns=data.columns)\n",
    "X_1=X_train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metric functions and functions for getting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def roc_auc_score(y_true, y_score, average=\"macro\"): #since there is no roc_auc_score in sklearn 0.12.1\n",
    "    if len(np.unique(y_true)) != 2:\n",
    "        raise ValueError(\"Only one class present in y_true. ROC AUC score\")\n",
    "    fpr, tpr, tresholds = roc_curve(y_true, y_score)\n",
    "    return auc(fpr, tpr)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotROC(X_test_proba,Y_test,label_string):\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, X_test_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
    "    fig=plt.figure()\n",
    "    plt.plot(fpr, tpr, color = c, label = label_string + ' (AUC = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC_{0}'.format(label_string))\n",
    "    plt.legend(loc=4,prop={'size':10})\n",
    "    plt.savefig('{0}.pdf'.format(label_string))\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lr_model1_GridSearch(no cross_validation)_baseline (Using text only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_subtrain, X_subtest, Y_subtrain, Y_subtest = train_test_split(X_train,X_train['Label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_subtrain.shape,X_subtest.shape,Y_subtrain.shape,Y_subtest.shape\n",
    "(584447, 31), (146112, 31), (584447,), (146112,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_subtrain=pd.DataFrame(X_subtrain,columns=X_train.columns)\n",
    "X_subtest=pd.DataFrame(X_subtest,columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Self_GridS(X_train, X_test, y_train, y_test):\n",
    "    auc_score={}\n",
    "    for b in parameters['vect__max_features']:\n",
    "        for c in parameters['vect__ngram_range']:\n",
    "            X_tr=X_train.copy()\n",
    "            X_te=X_test.copy()\n",
    "            vectorizer = CountVectorizer(max_features=b,ngram_range=c) \n",
    "            vectorizer.fit(X_tr)\n",
    "            X_tr= vectorizer.transform(X_tr)\n",
    "            X_te=vectorizer.transform(X_te)\n",
    "            for d in parameters['tfidf__use_idf']:\n",
    "                X_tr1=X_tr.copy()\n",
    "                X_te1=X_te.copy()\n",
    "                if d==True:\n",
    "                    tfmodel=TfidfTransformer(use_idf=d)\n",
    "                    tfmodel.fit(X_tr1)\n",
    "                    X_tr1=tfmodel.transform(X_tr1)\n",
    "                    X_te1=tfmodel.transform(X_te1)\n",
    "                for e in parameters['lr__alpha']:\n",
    "                    for f in parameters['lr__penalty']:\n",
    "                        lrmodel=SGDClassifier(loss='log',alpha=e,penalty=f)\n",
    "                        lrmodel.fit(X_tr1,y_train)\n",
    "                        y_predict=lrmodel.predict_proba(X_te1)[:,1]\n",
    "                        auc=roc_auc_score(y_test,y_predict)\n",
    "                        auc_score['b={0},c={1},d={2},e={3},f={4}'.format(b,c,d,e,f)]=auc\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_score_lr1=Self_(X_subtrain['text'], X_subtest['text'], Y_subtrain, Y_subtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('baseline_AUC.csv', 'w')\n",
    "for key, value in auc_score_lr1.items():\n",
    "    f.write('{0}, {1}\\n'.format(key,value))\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then see the best option is: \n",
    "b=None\tc=(1\t 2)\td=False\te=0.01\tf=l2\t0.693732339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(max_features=None,ngram_range=(1,2))\n",
    "vectorizer.fit(X_1)\n",
    "X_1=vectorizer.transform(X_1)\n",
    "X_test1=vectorizer.transform(X_test['text'].copy())\n",
    "X_subtrain_text=vectorizer.transform(X_subtrain['text'].copy())\n",
    "X_subtest_text=vectorizer.transform(X_subtest['text'].copy())\n",
    "lrmodel=SGDClassifier(loss='log',alpha=0.01,penalty='l2')\n",
    "lrmodel.fit(X_1,Y_train)\n",
    "prob1=lrmodel.predict_proba(X_test1)[:,1]\n",
    "prob1_train=lrmodel.predict_proba(X_1)[:,1]\n",
    "prob_train_forlr3=lrmodel.predict_proba(X_subtrain_text)[:,1]\n",
    "prob_test_forlr3=lrmodel.predict_proba(X_subtest_text)[:,1]\n",
    "np.savetxt('predict_proba_result_on_test_lr_baseline.txt',prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr1_auc= plotROC(prob1,Y_test, \"Baseline_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_lr1=pd.DataFrame(X_test['text'].copy())\n",
    "df_lr1['target']=X_test['Label']\n",
    "df_lr1['prediction']=prob1\n",
    "df_lr1['predict_class'] = pd.Series(df_lr1['prediction'] >= 0.5, dtype=int)\n",
    "wrong_base_shouldbe_1=df_lr1[np.logical_and(df_lr1['target']==1, df_lr1['predict_class']==0)]\n",
    "wrong_base_shouldbe_0=df_lr1[np.logical_and(df_lr1['target']==0, df_lr1['predict_class']==1)]\n",
    "wrong_base_shouldbe_1.to_csv('baseline_shouldbe1.csv')\n",
    "wrong_base_shouldbe_0.to_csv('baseline_shouldbe0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lr_model2_text_plus_rest (user/business_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### based on the performance on lr_model1, slightly extend the area of parameter\n",
    "parameters1 = {\n",
    "    'alpha':[10**i for i in range(-3, 4)],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Self_GridS2(X_train, X_test, y_train, y_test):\n",
    "    auc_score={}\n",
    "    X_train_other=sp.sparse.csr_matrix(X_train.drop(['text','Label'],1).astype(float))\n",
    "    X_test_other=sp.sparse.csr_matrix(X_test.drop(['text','Label'],1).astype(float))\n",
    "    X_train=sp.sparse.hstack([X_subtrain_text,X_train_other])\n",
    "    X_test=sp.sparse.hstack([X_subtest_text,X_test_other])\n",
    "    for a in parameters1['alpha']:\n",
    "        for b in parameters1['penalty']:\n",
    "            lrmodel=SGDClassifier(loss='log',alpha=a,penalty=b)\n",
    "            lrmodel.fit(X_train,y_train)\n",
    "            y_predict=lrmodel.predict_proba(X_test)[:,1]\n",
    "            auc=roc_auc_score(y_test,y_predict)\n",
    "            auc_score['a={0},b={1}'.format(a,b)]=auc\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_score_lr2=Self_GridS2(X_subtrain.copy(), X_subtest.copy(), Y_subtrain, Y_subtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('baseline_plusrest_AUC.csv', 'w')\n",
    "for key, value in auc_score_lr2.items():\n",
    "    f.write('{0}, {1}\\n'.format(key,value))\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=0.001,b=l2': 0.71789502413225736, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_2_text=X_1\n",
    "X_2_other=sp.sparse.csr_matrix(X_train.drop(['text','Label'],1).astype(float))\n",
    "X_2=sp.sparse.hstack([X_2_text,X_2_other]) #This is the variable set, suppose ‘a’ is the sparse matrix from CountVectorizer/balabala fit-transformed data.\n",
    "X_2=sp.sparse.csr_matrix(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_text=X_test1\n",
    "X_test_other=sp.sparse.csr_matrix(X_test.drop(['text','Label'],1).astype(float))\n",
    "X_test2=sp.sparse.hstack([X_test_text,X_test_other]) #This is the variable set, suppose ‘a’ is the sparse matrix from CountVectorizer/balabala fit-transformed data.\n",
    "X_test2=sp.sparse.csr_matrix(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrmodel=SGDClassifier(loss='log',alpha=0.001,penalty='l2')#pick up the best result\n",
    "lrmodel.fit(X_2,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob2=lrmodel.predict_proba(X_test2)[:,1]\n",
    "np.savetxt('predict_proba_result_lr_plusrest.txt',prob2)\n",
    "lr2_auc= plotROC(prob2,Y_test, \"Baseline_lr_plusrest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_lr2=pd.DataFrame(X_test.copy())#X_test is the original dataset\n",
    "df_lr2['prediction']=prob2\n",
    "df_lr2['predict_class'] = pd.Series(df_lr2['prediction'] >= 0.5, dtype=int)\n",
    "wrong_2_shouldbe_1=df_lr2[np.logical_and(df_lr2['Label']==1, df_lr2['predict_class']==0)]\n",
    "wrong_2_shouldbe_0=df_lr2[np.logical_and(df_lr2['Label']==0, df_lr2['predict_class']==1)]\n",
    "wrong_2_shouldbe_1.to_csv('baseline_plusrest_shouldbe1.csv')\n",
    "wrong_2_shouldbe_0.to_csv('baseline_plusrest_shouldbe0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lr_model3_loop_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters2 = {\n",
    "    'alpha':[10**i for i in range(-3, 4)],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Self_GridS3(X_1train, X_1test, y_train, y_test):\n",
    "    auc_score={}\n",
    "    X_train=X_1train\n",
    "    X_test=X_1test\n",
    "    X_train['Label_prob']=prob_train_forlr3\n",
    "    X_test['Label_prob']=prob_test_forlr3\n",
    "    X_train=X_train.drop(['text','Label'],1)\n",
    "    X_test=X_test.drop(['text','Label'],1)\n",
    "    X_train=sp.sparse.csr_matrix(X_train.astype(float))\n",
    "    X_test=sp.sparse.csr_matrix(X_test.astype(float))\n",
    "    for a in parameters2['alpha']:\n",
    "        for b in parameters2['penalty']:\n",
    "            lrmodel=SGDClassifier(loss='log',alpha=a,penalty=b)\n",
    "            lrmodel.fit(X_train,y_train)\n",
    "            y_predict=lrmodel.predict_proba(X_test)[:,1]\n",
    "            auc=roc_auc_score(y_test,y_predict)\n",
    "            auc_score['a={0},b={1}'.format(a,b)]=auc\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_score_lr3=Self_GridS3(X_subtrain.copy(), X_subtest.copy(), Y_subtrain, Y_subtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('baseline_loop_AUC.csv', 'w')\n",
    "for key, value in auc_score_lr3.items():\n",
    "    f.write('{0}, {1}\\n'.format(key,value))\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_3=X_train\n",
    "X_3['Label_prob']=prob1_train\n",
    "X_3=X_3.drop(['text','Label'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test3=X_test\n",
    "X_test3['Label_prob']=prob1\n",
    "X_test3=X_test3.drop(['text','Label'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrmodel=SGDClassifier(loss='log',alpha=0.001,penalty='l2')#pick up the best result\n",
    "lrmodel.fit(X_3,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob3=lrmodel.predict_proba(X_test3.astype(float))[:,1]   \n",
    "lr3_auc= plotROC(prob3,Y_test, \"Baseline_loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_lr3=pd.DataFrame(X_test3.copy())#X_test is the original dataset\n",
    "df_lr3['prediction']=prob3\n",
    "df_lr3['target']=X_test['Label']\n",
    "df_lr3['predict_class'] = pd.Series(df_lr3['prediction'] >= 0.5, dtype=int)\n",
    "wrong_3_shouldbe_1=df_lr3[np.logical_and(df_lr3['target']==1, df_lr3['predict_class']==0)]\n",
    "wrong_3_shouldbe_0=df_lr3[np.logical_and(df_lr3['target']==0, df_lr3['predict_class']==1)]\n",
    "wrong_3_shouldbe_1.to_csv('baseline_loop_shouldbe1.csv')\n",
    "wrong_3_shouldbe_0.to_csv('baseline_loop_shouldbe0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('results_lr_loop.txt', 'w')\n",
    "f.write('This is for the lr_baseline_loop.txt\\n')\n",
    "f.write('{0}\\n'.format(parameters2))\n",
    "f.write('For training models, The best AUC is {0}\\n'.format(git3.best_score_))\n",
    "for param_name in sorted(parameters2.keys()):\n",
    "    f.write(\"\\t%s: %r\\n\" % (param_name, best_para3[param_name]))\n",
    "f.write('for the training set, the auc is{0}\\n'.format(lr3_auc))\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then compare three models and pick up the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print lr1_auc, lr2_auc, lr3_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_vali=pd.read_csv('final_dummies_validation.csv')\n",
    "X_data_text=data['text'].copy()\n",
    "X_data.shape\n",
    "(913199,)\n",
    "X_vali_text=data_vali['text'].copy()\n",
    "Y_data=data['Label'].copy()\n",
    "Y_test=data_vali['Label'].copy()\n",
    "(391371,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(max_features=None,ngram_range=(1,2))\n",
    "vectorizer.fit(X_data_text)\n",
    "X_data_text=vectorizer.transform(X_data_text)\n",
    "X_data_other=sp.sparse.csr_matrix(data.drop(['text','Label'],1).astype(float))\n",
    "X_data=sp.sparse.hstack([X_data_text,X_data_other]) #This is the variable set, suppose ‘a’ is the sparse matrix from CountVectorizer/balabala fit-transformed data.\n",
    "X_data=sp.sparse.csr_matrix(X_data)\n",
    "X_vali_text=vectorizer.transform(X_vali_text)\n",
    "X_vali_other=sp.sparse.csr_matrix(data_vali.drop(['text','Label'],1).astype(float))\n",
    "X_vali=sp.sparse.hstack([X_vali_text,X_vali_other]) #This is the variable set, suppose ‘a’ is the sparse matrix from CountVectorizer/balabala fit-transformed data.\n",
    "X_vali=sp.sparse.csr_matrix(X_vali)\n",
    "lrmodel=SGDClassifier(loss='log',alpha=0.01,penalty='l2')\n",
    "lrmodel.fit(X_data,Y_data.values.astype(float))\n",
    "prob=lrmodel.predict_proba(X_vali.astype(float))[:,1]\n",
    "vali_auc= plotROC(prob,Y_test.values.astype(float), \"Validation\")\n",
    "proba_train=lrmodel.predict_proba(X_data)[:,1]\n",
    "train_auc= plotROC(proba_train,Y_data.values.astype(float), \"Validation_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### For all\n",
    "df_test=pd.read_csv('final_dummies_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other things like gridsearch and pipline are tried before, but not impropriate for such big amount of data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
